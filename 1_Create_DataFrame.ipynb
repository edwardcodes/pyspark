{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca73bc54-aa10-4519-a905-00ddb0727305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('abcdefghijklm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1292135-a055-49d4-8cb5-1c1e943f49c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mspark\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1266531b-2ab9-4c77-a78a-defcac0c618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bacd655-4afc-4bfd-b6d9-0f757a29c67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a05e3d-8d76-40d9-8ebc-87ba97c41843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_jwrapped',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " '_wrapped',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'conf',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'version']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f68093-73be-4249-a4ec-f8bf247385eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 2.1.0\n",
      "       Added verifySchema.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "        :class:`pandas.DataFrame`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      "        We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d72e639-4755-455a-838f-b06cb7b92d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([[1, 2], [3, 4]]).show() # just a df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ece2f2-1548-44ec-88a3-9debb84a5d09",
   "metadata": {},
   "source": [
    "#### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322b5a9b-aef5-4351-a8d8-87a41a1fcbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1| Edward|\n",
      "|  2|Praveen|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With schema\n",
    "\n",
    "df = spark.createDataFrame(data=[(1, 'Edward'), (2, 'Praveen')], schema=['id', 'name'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4f4413-798c-4872-bc6e-4ee90674edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f26c80-2d0e-48fc-aed1-67f7bfcfd8c6",
   "metadata": {},
   "source": [
    "#### Method 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "338cc35d-c677-4e89-9d18-541d9d4e7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StructType in module pyspark.sql.types:\n",
      "\n",
      "class StructType(DataType)\n",
      " |  StructType(fields=None)\n",
      " |  \n",
      " |  Struct type, consisting of a list of :class:`StructField`.\n",
      " |  \n",
      " |  This is the data type representing a :class:`Row`.\n",
      " |  \n",
      " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
      " |  A contained :class:`StructField` can be accessed by its name or position.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1[\"f1\"]\n",
      " |  StructField(f1,StringType,true)\n",
      " |  >>> struct1[0]\n",
      " |  StructField(f1,StringType,true)\n",
      " |  \n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
      " |  >>> struct1 == struct2\n",
      " |  False\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StructType\n",
      " |      DataType\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      Access fields by name or slice.\n",
      " |  \n",
      " |  __init__(self, fields=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate the fields\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of fields.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add(self, field, data_type=None, nullable=True, metadata=None)\n",
      " |      Construct a StructType by adding new elements to it, to define the schema.\n",
      " |      The method accepts either:\n",
      " |      \n",
      " |          a) A single parameter which is a StructField object.\n",
      " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
      " |             metadata(optional). The data_type parameter may be either a String or a\n",
      " |             DataType object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field : str or :class:`StructField`\n",
      " |          Either the name of the field or a StructField object\n",
      " |      data_type : :class:`DataType`, optional\n",
      " |          If present, the DataType of the StructField to create\n",
      " |      nullable : bool, optional\n",
      " |          Whether the field to add should be nullable (default True)\n",
      " |      metadata : dict, optional\n",
      " |          Any additional metadata (default None)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\n",
      " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |  \n",
      " |  fieldNames(self)\n",
      " |      Returns all field names in a list.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct.fieldNames()\n",
      " |      ['f1']\n",
      " |  \n",
      " |  fromInternal(self, obj)\n",
      " |      Converts an internal SQL object into a native Python object.\n",
      " |  \n",
      " |  jsonValue(self)\n",
      " |  \n",
      " |  needConversion(self)\n",
      " |      Does this type needs conversion between Python object and internal SQL object.\n",
      " |      \n",
      " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      " |  \n",
      " |  simpleString(self)\n",
      " |  \n",
      " |  toInternal(self, obj)\n",
      " |      Converts a Python object into an internal SQL object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromJson(json) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DataType:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  json(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from DataType:\n",
      " |  \n",
      " |  typeName() from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DataType:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "help(StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "559b0cd1-4a51-4812-90cf-6e3a892f2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1| Edward|\n",
      "|  2|Praveen|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passing Customized Defined Schema\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [\n",
    "    (1, 'Edward'), (2, 'Praveen')\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('name', StringType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6988a67b-dded-45ae-8523-3f1e4c656350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295263e-4683-4733-99a1-33127d7d2519",
   "metadata": {},
   "source": [
    "#### Method 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab8a9e8-ce32-4457-ae9a-bd60f666f85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1| Edward|\n",
      "|  2|Praveen|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passing data and schema as key, value pairs\n",
    "\n",
    "dict_data = [{'id':1, 'name':'Edward'}, {'id':2, 'name':'Praveen'}]\n",
    "\n",
    "df = spark.createDataFrame(dict_data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a49bfbd0-86c5-4ed8-8fab-418117ccc077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a4fac-268d-412d-b609-4014139396c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
