{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a8acdf-d505-4e16-ace8-904a1a14c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d793f2-3499-4dfa-80e3-5cb7ee7783ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None, unescapedQuoteHandling=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str or list\n",
      "        string, or list of strings, for input path(s),\n",
      "        or RDD of Strings storing CSV rows.\n",
      "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "        an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    sep : str, optional\n",
      "        sets a separator (one or more characters) for each field and value. If None is\n",
      "        set, it uses the default value, ``,``.\n",
      "    encoding : str, optional\n",
      "        decodes the CSV files by the given encoding type. If None is set,\n",
      "        it uses the default value, ``UTF-8``.\n",
      "    quote : str, optional\n",
      "        sets a single character used for escaping quoted values where the\n",
      "        separator can be part of the value. If None is set, it uses the default\n",
      "        value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      "        empty string.\n",
      "    escape : str, optional\n",
      "        sets a single character used for escaping quotes inside an already\n",
      "        quoted value. If None is set, it uses the default value, ``\\``.\n",
      "    comment : str, optional\n",
      "        sets a single character used for skipping lines beginning with this\n",
      "        character. By default (None), it is disabled.\n",
      "    header : str or bool, optional\n",
      "        uses the first line as names of columns. If None is set, it uses the\n",
      "        default value, ``false``.\n",
      "    \n",
      "        .. note:: if the given path is a RDD of Strings, this header\n",
      "            option will remove all lines same with the header if exists.\n",
      "    \n",
      "    inferSchema : str or bool, optional\n",
      "        infers the input schema automatically from data. It requires one extra\n",
      "        pass over the data. If None is set, it uses the default value, ``false``.\n",
      "    enforceSchema : str or bool, optional\n",
      "        If it is set to ``true``, the specified or inferred schema will be\n",
      "        forcibly applied to datasource files, and headers in CSV files will be\n",
      "        ignored. If the option is set to ``false``, the schema will be\n",
      "        validated against all headers in CSV files or the first header in RDD\n",
      "        if the ``header`` option is set to ``true``. Field names in the schema\n",
      "        and column names in CSV headers are checked by their positions\n",
      "        taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      "        ``true`` is used by default. Though the default value is ``true``,\n",
      "        it is recommended to disable the ``enforceSchema`` option\n",
      "        to avoid incorrect results.\n",
      "    ignoreLeadingWhiteSpace : str or bool, optional\n",
      "        A flag indicating whether or not leading whitespaces from\n",
      "        values being read should be skipped. If None is set, it\n",
      "        uses the default value, ``false``.\n",
      "    ignoreTrailingWhiteSpace : str or bool, optional\n",
      "        A flag indicating whether or not trailing whitespaces from\n",
      "        values being read should be skipped. If None is set, it\n",
      "        uses the default value, ``false``.\n",
      "    nullValue : str, optional\n",
      "        sets the string representation of a null value. If None is set, it uses\n",
      "        the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      "        applies to all supported types including the string type.\n",
      "    nanValue : str, optional\n",
      "        sets the string representation of a non-number value. If None is set, it\n",
      "        uses the default value, ``NaN``.\n",
      "    positiveInf : str, optional\n",
      "        sets the string representation of a positive infinity value. If None\n",
      "        is set, it uses the default value, ``Inf``.\n",
      "    negativeInf : str, optional\n",
      "        sets the string representation of a negative infinity value. If None\n",
      "        is set, it uses the default value, ``Inf``.\n",
      "    dateFormat : str, optional\n",
      "        sets the string that indicates a date format. Custom date formats\n",
      "        follow the formats at\n",
      "        `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      "        This applies to date type. If None is set, it uses the\n",
      "        default value, ``yyyy-MM-dd``.\n",
      "    timestampFormat : str, optional\n",
      "        sets the string that indicates a timestamp format.\n",
      "        Custom date formats follow the formats at\n",
      "        `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      "        This applies to timestamp type. If None is set, it uses the\n",
      "        default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      "    maxColumns : str or int, optional\n",
      "        defines a hard limit of how many columns a record can have. If None is\n",
      "        set, it uses the default value, ``20480``.\n",
      "    maxCharsPerColumn : str or int, optional\n",
      "        defines the maximum number of characters allowed for any given\n",
      "        value being read. If None is set, it uses the default value,\n",
      "        ``-1`` meaning unlimited length.\n",
      "    maxMalformedLogPerPartition : str or int, optional\n",
      "        this parameter is no longer used since Spark 2.2.0.\n",
      "        If specified, it is ignored.\n",
      "    mode : str, optional\n",
      "        allows a mode for dealing with corrupt records during parsing. If None is\n",
      "        set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
      "        parse only required columns in CSV under column pruning. Therefore, corrupt\n",
      "        records can be different based on required set of fields. This behavior can\n",
      "        be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
      "        (enabled by default).\n",
      "    \n",
      "        * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string \\\n",
      "          into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\n",
      "          fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      "          field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      "          schema does not have the field, it drops corrupt records during parsing. \\\n",
      "          A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      "          When it meets a record having fewer tokens than the length of the schema, \\\n",
      "          sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      "          length of the schema, it drops extra tokens.\n",
      "        * ``DROPMALFORMED``: ignores the whole corrupted records.\n",
      "        * ``FAILFAST``: throws an exception when it meets corrupted records.\n",
      "    \n",
      "    columnNameOfCorruptRecord : str, optional\n",
      "        allows renaming the new field having malformed string\n",
      "        created by ``PERMISSIVE`` mode. This overrides\n",
      "        ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "        it uses the value specified in\n",
      "        ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    multiLine : str or bool, optional\n",
      "        parse records, which may span multiple lines. If None is\n",
      "        set, it uses the default value, ``false``.\n",
      "    charToEscapeQuoteEscaping : str, optional\n",
      "        sets a single character used for escaping the escape for\n",
      "        the quote character. If None is set, the default value is\n",
      "        escape character when escape and quote characters are\n",
      "        different, ``\\0`` otherwise.\n",
      "    samplingRatio : str or float, optional\n",
      "        defines fraction of rows used for schema inferring.\n",
      "        If None is set, it uses the default value, ``1.0``.\n",
      "    emptyValue : str, optional\n",
      "        sets the string representation of an empty value. If None is set, it uses\n",
      "        the default value, empty string.\n",
      "    locale : str, optional\n",
      "        sets a locale as language tag in IETF BCP 47 format. If None is set,\n",
      "        it uses the default value, ``en-US``. For instance, ``locale`` is used while\n",
      "        parsing dates and timestamps.\n",
      "    lineSep : str, optional\n",
      "        defines the line separator that should be used for parsing. If None is\n",
      "        set, it covers all ``\\\\r``, ``\\\\r\\\\n`` and ``\\\\n``.\n",
      "        Maximum length is 1 character.\n",
      "    pathGlobFilter : str or bool, optional\n",
      "        an optional glob pattern to only include files with paths matching\n",
      "        the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      "        It does not change the behavior of\n",
      "        `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      "    recursiveFileLookup : str or bool, optional\n",
      "        recursively scan a directory for files. Using this option disables\n",
      "        `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      "    \n",
      "        modification times occurring before the specified time. The provided timestamp\n",
      "        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      "    modifiedBefore (batch only) : an optional timestamp to only include files with\n",
      "        modification times occurring before the specified time. The provided timestamp\n",
      "        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      "    modifiedAfter (batch only) : an optional timestamp to only include files with\n",
      "        modification times occurring after the specified time. The provided timestamp\n",
      "        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      "    unescapedQuoteHandling : str, optional\n",
      "        defines how the CsvParser will handle values with unescaped quotes. If None is\n",
      "        set, it uses the default value, ``STOP_AT_DELIMITER``.\n",
      "    \n",
      "        * ``STOP_AT_CLOSING_QUOTE``: If unescaped quotes are found in the input, accumulate\n",
      "          the quote character and proceed parsing the value as a quoted value, until a closing\n",
      "          quote is found.\n",
      "        * ``BACK_TO_DELIMITER``: If unescaped quotes are found in the input, consider the value\n",
      "          as an unquoted value. This will make the parser accumulate all characters of the current\n",
      "          parsed value until the delimiter is found. If no delimiter is found in the value, the\n",
      "          parser will continue accumulating characters from the input until a delimiter or line\n",
      "          ending is found.\n",
      "        * ``STOP_AT_DELIMITER``: If unescaped quotes are found in the input, consider the value\n",
      "          as an unquoted value. This will make the parser accumulate all characters until the\n",
      "          delimiter or a line ending is found in the input.\n",
      "        * ``SKIP_VALUE``: If unescaped quotes are found in the input, the content parsed\n",
      "          for the given value will be skipped and the value set in nullValue will be produced\n",
      "          instead.\n",
      "        * ``RAISE_ERROR``: If unescaped quotes are found in the input, a TextParsingException\n",
      "          will be thrown.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7233e015-7e13-427b-8c5e-53d93c38d37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function display in module IPython.core.display_functions:\n",
      "\n",
      "display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, raw=False, clear=False, **kwargs)\n",
      "    Display a Python object in all frontends.\n",
      "    \n",
      "    By default all representations will be computed and sent to the frontends.\n",
      "    Frontends can decide which representation is used and how.\n",
      "    \n",
      "    In terminal IPython this will be similar to using :func:`print`, for use in richer\n",
      "    frontends see Jupyter notebook examples with rich display logic.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *objs : object\n",
      "        The Python objects to display.\n",
      "    raw : bool, optional\n",
      "        Are the objects to be displayed already mimetype-keyed dicts of raw display data,\n",
      "        or Python objects that need to be formatted before display? [default: False]\n",
      "    include : list, tuple or set, optional\n",
      "        A list of format type strings (MIME types) to include in the\n",
      "        format data dict. If this is set *only* the format types included\n",
      "        in this list will be computed.\n",
      "    exclude : list, tuple or set, optional\n",
      "        A list of format type strings (MIME types) to exclude in the format\n",
      "        data dict. If this is set all format types will be computed,\n",
      "        except for those included in this argument.\n",
      "    metadata : dict, optional\n",
      "        A dictionary of metadata to associate with the output.\n",
      "        mime-type keys in this dictionary will be associated with the individual\n",
      "        representation formats, if they exist.\n",
      "    transient : dict, optional\n",
      "        A dictionary of transient data to associate with the output.\n",
      "        Data in this dict should not be persisted to files (e.g. notebooks).\n",
      "    display_id : str, bool optional\n",
      "        Set an id for the display.\n",
      "        This id can be used for updating this display area later via update_display.\n",
      "        If given as `True`, generate a new `display_id`\n",
      "    clear : bool, optional\n",
      "        Should the output area be cleared before displaying anything? If True,\n",
      "        this will wait for additional output before clearing. [default: False]\n",
      "    **kwargs : additional keyword-args, optional\n",
      "        Additional keyword-arguments are passed through to the display publisher.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    handle: DisplayHandle\n",
      "        Returns a handle on updatable displays for use with :func:`update_display`,\n",
      "        if `display_id` is given. Returns :any:`None` if no `display_id` is given\n",
      "        (default).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> class Json(object):\n",
      "    ...     def __init__(self, json):\n",
      "    ...         self.json = json\n",
      "    ...     def _repr_pretty_(self, pp, cycle):\n",
      "    ...         import json\n",
      "    ...         pp.text(json.dumps(self.json, indent=2))\n",
      "    ...     def __repr__(self):\n",
      "    ...         return str(self.json)\n",
      "    ...\n",
      "    \n",
      "    >>> d = Json({1:2, 3: {4:5}})\n",
      "    \n",
      "    >>> print(d)\n",
      "    {1: 2, 3: {4: 5}}\n",
      "    \n",
      "    >>> display(d)\n",
      "    {\n",
      "      \"1\": 2,\n",
      "      \"3\": {\n",
      "        \"4\": 5\n",
      "      }\n",
      "    }\n",
      "    \n",
      "    >>> def int_formatter(integer, pp, cycle):\n",
      "    ...     pp.text('I'*integer)\n",
      "    \n",
      "    >>> plain = get_ipython().display_formatter.formatters['text/plain']\n",
      "    >>> plain.for_type(int, int_formatter)\n",
      "    <function _repr_pprint at 0x...>\n",
      "    >>> display(7-5)\n",
      "    II\n",
      "    \n",
      "    >>> del plain.type_printers[int]\n",
      "    >>> display(7-5)\n",
      "    2\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    :func:`update_display`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In Python, objects can declare their textual representation using the\n",
      "    `__repr__` method. IPython expands on this idea and allows objects to declare\n",
      "    other, rich representations including:\n",
      "    \n",
      "      - HTML\n",
      "      - JSON\n",
      "      - PNG\n",
      "      - JPEG\n",
      "      - SVG\n",
      "      - LaTeX\n",
      "    \n",
      "    A single object can declare some or all of these representations; all are\n",
      "    handled by IPython's display system.\n",
      "    \n",
      "    The main idea of the first approach is that you have to implement special\n",
      "    display methods when you define your class, one for each representation you\n",
      "    want to use. Here is a list of the names of the special methods and the\n",
      "    values they must return:\n",
      "    \n",
      "      - `_repr_html_`: return raw HTML as a string, or a tuple (see below).\n",
      "      - `_repr_json_`: return a JSONable dict, or a tuple (see below).\n",
      "      - `_repr_jpeg_`: return raw JPEG data, or a tuple (see below).\n",
      "      - `_repr_png_`: return raw PNG data, or a tuple (see below).\n",
      "      - `_repr_svg_`: return raw SVG data as a string, or a tuple (see below).\n",
      "      - `_repr_latex_`: return LaTeX commands in a string surrounded by \"$\",\n",
      "                        or a tuple (see below).\n",
      "      - `_repr_mimebundle_`: return a full mimebundle containing the mapping\n",
      "                             from all mimetypes to data.\n",
      "                             Use this for any mime-type not listed above.\n",
      "    \n",
      "    The above functions may also return the object's metadata alonside the\n",
      "    data.  If the metadata is available, the functions will return a tuple\n",
      "    containing the data and metadata, in that order.  If there is no metadata\n",
      "    available, then the functions will return the data only.\n",
      "    \n",
      "    When you are directly writing your own classes, you can adapt them for\n",
      "    display in IPython by following the above approach. But in practice, you\n",
      "    often need to work with existing classes that you can't easily modify.\n",
      "    \n",
      "    You can refer to the documentation on integrating with the display system in\n",
      "    order to register custom formatters for already existing types\n",
      "    (:ref:`integrating_rich_display`).\n",
      "    \n",
      "    .. versionadded:: 5.4 display available without import\n",
      "    .. versionadded:: 6.1 display available without import\n",
      "    \n",
      "    Since IPython 5.4 and 6.1 :func:`display` is automatically made available to\n",
      "    the user without import. If you are using display in a document that might\n",
      "    be used in a pure python context or with older version of IPython, use the\n",
      "    following import at the top of your file::\n",
      "    \n",
      "        from IPython.display import display\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177947be-5e69-446d-86c7-0c45bafa2c48",
   "metadata": {},
   "source": [
    "#### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "333ac3bd-cb8c-42e4-a909-c05344cd1d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[state: string, capital: string, language: string, _c3: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(path='/user/bigdatapedia/data/India.csv',header=True)\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fc8dc8e-b05e-48d0-9e6f-f6e1fd70bc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "031e98b1-090a-458e-98c4-b047e1b468dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+----+\n",
      "|            state|             capital|            language| _c3|\n",
      "+-----------------+--------------------+--------------------+----+\n",
      "|   Andhra Pradesh|           Amaravati|              Telugu|null|\n",
      "|Arunachal Pradesh|            Itanagar|             English|null|\n",
      "|            Assam|              Dispur|            Assamese|null|\n",
      "|            Bihar|               Patna|               Hindi|null|\n",
      "|     Chhattisgarh|              Raipur|       Chhattisgarhi|null|\n",
      "|              Goa|              Panaji|             Konkani|null|\n",
      "|          Gujarat|         Gandhinagar|            Gujarati|null|\n",
      "|          Haryana|          Chandigarh|            Haryanvi|null|\n",
      "| Himachal Pradesh|              Shimla|               Hindi|null|\n",
      "|Jammu and Kashmir|Srinagar (summer)...|                Urdu|null|\n",
      "|        Jharkhand|              Ranchi|               Hindi|null|\n",
      "|        Karnataka|           Bengaluru|             Kannada|null|\n",
      "|           Kerala|          Trivandrum|           Malayalam|null|\n",
      "|   Madhya Pradesh|              Bhopal|               Hindi|null|\n",
      "|      Maharashtra|              Mumbai|             Marathi|null|\n",
      "|          Manipur|              Imphal|Meiteilon (Manipuri)|null|\n",
      "|        Meghalaya|            Shillong|             English|null|\n",
      "|          Mizoram|              Aizawl|                Mizo|null|\n",
      "|         Nagaland|              Kohima|             English|null|\n",
      "|           Odisha|         Bhubaneswar|               Oriya|null|\n",
      "+-----------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:35:07 WARN csv.CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: state, capital, language, \n",
      " Schema: state, capital, language, _c3\n",
      "Expected: _c3 but found: \n",
      "CSV file: hdfs://hdfs-bigdatapedia:9000/user/bigdatapedia/data/India.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6817c77a-2e98-4276-9d4c-48815ae22f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping _c3 column\n",
    "df = df.drop('_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9570adf-a407-48d1-ae36-cf39585ccbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+\n",
      "|            state|             capital|            language|\n",
      "+-----------------+--------------------+--------------------+\n",
      "|   Andhra Pradesh|           Amaravati|              Telugu|\n",
      "|Arunachal Pradesh|            Itanagar|             English|\n",
      "|            Assam|              Dispur|            Assamese|\n",
      "|            Bihar|               Patna|               Hindi|\n",
      "|     Chhattisgarh|              Raipur|       Chhattisgarhi|\n",
      "|              Goa|              Panaji|             Konkani|\n",
      "|          Gujarat|         Gandhinagar|            Gujarati|\n",
      "|          Haryana|          Chandigarh|            Haryanvi|\n",
      "| Himachal Pradesh|              Shimla|               Hindi|\n",
      "|Jammu and Kashmir|Srinagar (summer)...|                Urdu|\n",
      "|        Jharkhand|              Ranchi|               Hindi|\n",
      "|        Karnataka|           Bengaluru|             Kannada|\n",
      "|           Kerala|          Trivandrum|           Malayalam|\n",
      "|   Madhya Pradesh|              Bhopal|               Hindi|\n",
      "|      Maharashtra|              Mumbai|             Marathi|\n",
      "|          Manipur|              Imphal|Meiteilon (Manipuri)|\n",
      "|        Meghalaya|            Shillong|             English|\n",
      "|          Mizoram|              Aizawl|                Mizo|\n",
      "|         Nagaland|              Kohima|             English|\n",
      "|           Odisha|         Bhubaneswar|               Oriya|\n",
      "+-----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28657015-c443-4a56-8c94-a61fbf9d92f9",
   "metadata": {},
   "source": [
    "#### Method 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2851331-0a17-4531-bbe2-eddc3677082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format('csv').option(key='header',value=True).load(path='/user/bigdatapedia/data/India.csv')\n",
    "df2 = df2.drop('_c3')\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8714547c-a408-47af-af79-a918e8bf1f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+\n",
      "|            state|             capital|            language|\n",
      "+-----------------+--------------------+--------------------+\n",
      "|   Andhra Pradesh|           Amaravati|              Telugu|\n",
      "|Arunachal Pradesh|            Itanagar|             English|\n",
      "|            Assam|              Dispur|            Assamese|\n",
      "|            Bihar|               Patna|               Hindi|\n",
      "|     Chhattisgarh|              Raipur|       Chhattisgarhi|\n",
      "|              Goa|              Panaji|             Konkani|\n",
      "|          Gujarat|         Gandhinagar|            Gujarati|\n",
      "|          Haryana|          Chandigarh|            Haryanvi|\n",
      "| Himachal Pradesh|              Shimla|               Hindi|\n",
      "|Jammu and Kashmir|Srinagar (summer)...|                Urdu|\n",
      "|        Jharkhand|              Ranchi|               Hindi|\n",
      "|        Karnataka|           Bengaluru|             Kannada|\n",
      "|           Kerala|          Trivandrum|           Malayalam|\n",
      "|   Madhya Pradesh|              Bhopal|               Hindi|\n",
      "|      Maharashtra|              Mumbai|             Marathi|\n",
      "|          Manipur|              Imphal|Meiteilon (Manipuri)|\n",
      "|        Meghalaya|            Shillong|             English|\n",
      "|          Mizoram|              Aizawl|                Mizo|\n",
      "|         Nagaland|              Kohima|             English|\n",
      "|           Odisha|         Bhubaneswar|               Oriya|\n",
      "+-----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac30650-f675-465f-8cee-ab8fab251715",
   "metadata": {},
   "source": [
    "### Reading multiple CSVs which have same schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f81e34-a4ad-40f9-a0dd-c468d1f6dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multiple = spark.read.csv(path=['/user/bigdatapedia/data/India.csv','/user/bigdatapedia/data/America.csv'],header=True)\n",
    "df_multiple = df_multiple.drop('_c3')\n",
    "df_multiple.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c70d4c8-51b7-4d42-b6dc-c749e52228ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 09:07:59 WARN csv.CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 4, schema size: 3\n",
      "CSV file: hdfs://hdfs-bigdatapedia:9000/user/bigdatapedia/data/India.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(state='Alaska', capital='Juneau', language='English'),\n",
       " Row(state='Alabama', capital='Montgomery', language='English'),\n",
       " Row(state='Arizona', capital='Phoenix', language='English'),\n",
       " Row(state='Arkansas', capital='Little Rock', language='English'),\n",
       " Row(state='California', capital='Sacramento', language='English'),\n",
       " Row(state='Colorado', capital='Denver', language='English'),\n",
       " Row(state='Connecticut', capital='Hartford', language='English'),\n",
       " Row(state='Delaware', capital='Dover', language='English'),\n",
       " Row(state='Florida', capital='Tallahassee', language='English'),\n",
       " Row(state='Gregoria', capital='Atlanta', language='English'),\n",
       " Row(state='Hawaii', capital='Honolulu', language='English'),\n",
       " Row(state='Idaho', capital='Boise', language='English'),\n",
       " Row(state='Illinois', capital='Spring field', language='English'),\n",
       " Row(state='Indiana', capital='Indianapolis', language='English'),\n",
       " Row(state='Iowa', capital='Des Moines', language='English'),\n",
       " Row(state='Kansas', capital='Topeka', language='English'),\n",
       " Row(state='Kentucky', capital='Frankfort', language='English'),\n",
       " Row(state='Maine', capital='Augusta', language='English'),\n",
       " Row(state='Maryland', capital='Annapolis', language='English'),\n",
       " Row(state='Massachusetts', capital='Boston', language='English')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_multiple.collect()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "226f971f-2b40-496b-97ed-0c51deca60cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_multiple.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fb6f7f6-65fb-4297-b2dc-cd294c587b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multiple.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b78ab1-f88f-4ace-ab82-fe59558f31f0",
   "metadata": {},
   "source": [
    "***What if we want to define datatypes for columns instead of spark?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32d1988e-c492-41d2-96b4-01e48caaaacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"capital\", StringType(), True),\n",
    "        StructField(\"language\", StringType(), True)\n",
    "    ]\n",
    "                   )\n",
    "\n",
    "df_schema = spark.read.csv(path=['/user/bigdatapedia/data/India.csv','/user/bigdatapedia/data/America.csv'],\n",
    "                           schema=schema,\n",
    "                           header=True)\n",
    "\n",
    "# Removing unwanted column\n",
    "df_schema = df_schema.drop('_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd3cbac0-6bb7-49af-971f-e24aabe8c6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- capital: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d90f42-a170-44ef-a8d3-b34ba64587d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4fe392-10dc-4455-b727-a530acca8a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338841b0-cc28-4322-b37c-5b3eff7cdb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced85bd-83ed-49e0-84cc-6679aa3c425f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
