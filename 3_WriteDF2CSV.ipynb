{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca386bd-fd63-4e35-800d-1bf899070db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e6c1a9-6390-44c5-84fe-9ec889aab053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c816571a-c44a-403e-abe6-8f2f38719f80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataFrameWriter in module pyspark.sql.readwriter:\n",
      "\n",
      "class DataFrameWriter(OptionUtils)\n",
      " |  DataFrameWriter(df)\n",
      " |  \n",
      " |  Interface used to write a :class:`DataFrame` to external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameWriter\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, df)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  bucketBy(self, numBuckets, col, *cols)\n",
      " |      Buckets the output by the given columns.If specified,\n",
      " |      the output is laid out on the file system similar to Hive's bucketing scheme.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numBuckets : int\n",
      " |          the number of buckets to save\n",
      " |      col : str, list or tuple\n",
      " |          a name of a column, or a list of names.\n",
      " |      cols : str\n",
      " |          additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Applicable for file-based data sources in combination with\n",
      " |      :py:meth:`DataFrameWriter.saveAsTable`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> (df.write.format('parquet')  # doctest: +SKIP\n",
      " |      ...     .bucketBy(100, 'year', 'month')\n",
      " |      ...     .mode(\"overwrite\")\n",
      " |      ...     .saveAsTable('bucketed_table'))\n",
      " |  \n",
      " |  csv(self, path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None)\n",
      " |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      " |              exists.\n",
      " |      \n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      sep : str, optional\n",
      " |          sets a separator (one or more characters) for each field and value. If None is\n",
      " |          set, it uses the default value, ``,``.\n",
      " |      quote : str, optional\n",
      " |          sets a single character used for escaping quoted values where the\n",
      " |          separator can be part of the value. If None is set, it uses the default\n",
      " |          value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n",
      " |      escape : str, optional\n",
      " |          sets a single character used for escaping quotes inside an already\n",
      " |          quoted value. If None is set, it uses the default value, ``\\``\n",
      " |      escapeQuotes : str or bool, optional\n",
      " |          a flag indicating whether values containing quotes should always\n",
      " |          be enclosed in quotes. If None is set, it uses the default value\n",
      " |          ``true``, escaping all values containing a quote character.\n",
      " |      quoteAll : str or bool, optional\n",
      " |          a flag indicating whether all values should always be enclosed in\n",
      " |          quotes. If None is set, it uses the default value ``false``,\n",
      " |          only escaping values containing a quote character.\n",
      " |      header : str or bool, optional\n",
      " |          writes the names of columns as the first line. If None is set, it uses\n",
      " |          the default value, ``false``.\n",
      " |      nullValue : str, optional\n",
      " |          sets the string representation of a null value. If None is set, it uses\n",
      " |          the default value, empty string.\n",
      " |      dateFormat : str, optional\n",
      " |          sets the string that indicates a date format. Custom date formats follow\n",
      " |          the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to date type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd``.\n",
      " |      timestampFormat : str, optional\n",
      " |          sets the string that indicates a timestamp format.\n",
      " |          Custom date formats follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to timestamp type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      " |      ignoreLeadingWhiteSpace : str or bool, optional\n",
      " |          a flag indicating whether or not leading whitespaces from\n",
      " |          values being written should be skipped. If None is set, it\n",
      " |          uses the default value, ``true``.\n",
      " |      ignoreTrailingWhiteSpace : str or bool, optional\n",
      " |          a flag indicating whether or not trailing whitespaces from\n",
      " |          values being written should be skipped. If None is set, it\n",
      " |          uses the default value, ``true``.\n",
      " |      charToEscapeQuoteEscaping : str, optional\n",
      " |          sets a single character used for escaping the escape for\n",
      " |          the quote character. If None is set, the default value is\n",
      " |          escape character when escape and quote characters are\n",
      " |          different, ``\\0`` otherwise..\n",
      " |      encoding : str, optional\n",
      " |          sets the encoding (charset) of saved csv files. If None is set,\n",
      " |          the default UTF-8 charset will be used.\n",
      " |      emptyValue : str, optional\n",
      " |          sets the string representation of an empty value. If None is set, it uses\n",
      " |          the default value, ``\"\"``.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\\\n``. Maximum length is 1 character.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  format(self, source)\n",
      " |      Specifies the underlying output data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  insertInto(self, tableName, overwrite=None)\n",
      " |      Inserts the content of the :class:`DataFrame` to the specified table.\n",
      " |      \n",
      " |      It requires that the schema of the :class:`DataFrame` is the same as the\n",
      " |      schema of the table.\n",
      " |      \n",
      " |      Optionally overwriting any existing data.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  jdbc(self, url, table, mode=None, properties=None)\n",
      " |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      url : str\n",
      " |          a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      " |      table : str\n",
      " |          Name of the table in the external database.\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      properties : dict\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |  \n",
      " |  json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None, ignoreNullFields=None)\n",
      " |      Saves the content of the :class:`DataFrame` in JSON format\n",
      " |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      " |      specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      dateFormat : str, optional\n",
      " |          sets the string that indicates a date format. Custom date formats\n",
      " |          follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to date type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd``.\n",
      " |      timestampFormat : str, optional\n",
      " |          sets the string that indicates a timestamp format.\n",
      " |          Custom date formats follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to timestamp type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      " |      encoding : str, optional\n",
      " |          specifies encoding (charset) of saved json files. If None is set,\n",
      " |          the default UTF-8 charset will be used.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\n``.\n",
      " |      ignoreNullFields : str or bool, optional\n",
      " |          Whether to ignore null fields when generating JSON objects.\n",
      " |          If None is set, it uses the default value, ``true``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  mode(self, saveMode)\n",
      " |      Specifies the behavior when data or table already exists.\n",
      " |      \n",
      " |      Options include:\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  option(self, key, value)\n",
      " |      Adds an output option for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for writing files:\n",
      " |          * ``timeZone``: sets the string that indicates a time zone ID to be used to format\n",
      " |              timestamps in the JSON/CSV datasources or partition values. The following\n",
      " |              formats of `timeZone` are supported:\n",
      " |      \n",
      " |              * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      " |              * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      " |      \n",
      " |              Other short names like 'CST' are not recommended to use because they can be\n",
      " |              ambiguous. If it isn't set, the current value of the SQL config\n",
      " |              ``spark.sql.session.timeZone`` is used by default.\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options)\n",
      " |      Adds output options for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for writing files:\n",
      " |          * ``timeZone``: sets the string that indicates a time zone ID to be used to format\n",
      " |              timestamps in the JSON/CSV datasources or partition values. The following\n",
      " |              formats of `timeZone` are supported:\n",
      " |      \n",
      " |              * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      " |              * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      " |      \n",
      " |              Other short names like 'CST' are not recommended to use because they can be\n",
      " |              ambiguous. If it isn't set, the current value of the SQL config\n",
      " |              ``spark.sql.session.timeZone`` is used by default.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path, mode=None, partitionBy=None, compression=None)\n",
      " |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : str or list, optional\n",
      " |          names of partitioning columns\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, snappy, zlib, and lzo).\n",
      " |          This will override ``orc.compress`` and\n",
      " |          ``spark.sql.orc.compression.codec``. If None is set, it uses the value\n",
      " |          specified in ``spark.sql.orc.compression.codec``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  parquet(self, path, mode=None, partitionBy=None, compression=None)\n",
      " |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : str or list, optional\n",
      " |          names of partitioning columns\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n",
      " |          lzo, brotli, lz4, and zstd). This will override\n",
      " |          ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n",
      " |          value specified in ``spark.sql.parquet.compression.codec``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  partitionBy(self, *cols)\n",
      " |      Partitions the output by the given columns on the file system.\n",
      " |      \n",
      " |      If specified, the output is laid out on the file system similar\n",
      " |      to Hive's partitioning scheme.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str or list\n",
      " |          name of columns\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  save(self, path=None, format=None, mode=None, partitionBy=None, **options)\n",
      " |      Saves the contents of the :class:`DataFrame` to a data source.\n",
      " |      \n",
      " |      The data source is specified by the ``format`` and a set of ``options``.\n",
      " |      If ``format`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, optional\n",
      " |          the path in a Hadoop supported file system\n",
      " |      format : str, optional\n",
      " |          the format used to save\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : list, optional\n",
      " |          names of partitioning columns\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.mode(\"append\").save(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  saveAsTable(self, name, format=None, mode=None, partitionBy=None, **options)\n",
      " |      Saves the content of the :class:`DataFrame` as the specified table.\n",
      " |      \n",
      " |      In the case the table already exists, behavior of this function depends on the\n",
      " |      save mode, specified by the `mode` function (default to throwing an exception).\n",
      " |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
      " |      the same as that of the existing table.\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          the table name\n",
      " |      format : str, optional\n",
      " |          the format used to save\n",
      " |      mode : str, optional\n",
      " |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
      " |      partitionBy : str or list\n",
      " |          names of partitioning columns\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |  \n",
      " |  sortBy(self, col, *cols)\n",
      " |      Sorts the output in each bucket by the given columns on the file system.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : str, tuple or list\n",
      " |          a name of a column, or a list of names.\n",
      " |      cols : str\n",
      " |          additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> (df.write.format('parquet')  # doctest: +SKIP\n",
      " |      ...     .bucketBy(100, 'year', 'month')\n",
      " |      ...     .sortBy('day')\n",
      " |      ...     .mode(\"overwrite\")\n",
      " |      ...     .saveAsTable('sorted_bucketed_table'))\n",
      " |  \n",
      " |  text(self, path, compression=None, lineSep=None)\n",
      " |      Saves the content of the DataFrame in a text file at the specified path.\n",
      " |      The text files will be encoded as UTF-8.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\n``.\n",
      " |      \n",
      " |      The DataFrame must have only one column that is of string type.\n",
      " |      Each row becomes a new line in the output file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataFrameWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62603fc8-5a6b-4b2e-b408-a96efdc5a050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.dataframe in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.dataframe\n",
      "\n",
      "DESCRIPTION\n",
      "    # Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "    # contributor license agreements.  See the NOTICE file distributed with\n",
      "    # this work for additional information regarding copyright ownership.\n",
      "    # The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "    # (the \"License\"); you may not use this file except in compliance with\n",
      "    # the License.  You may obtain a copy of the License at\n",
      "    #\n",
      "    #    http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "    #\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        DataFrameNaFunctions\n",
      "        DataFrameStatFunctions\n",
      "    pyspark.sql.pandas.conversion.PandasConversionMixin(builtins.object)\n",
      "        DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "    pyspark.sql.pandas.map_ops.PandasMapOpsMixin(builtins.object)\n",
      "        DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "    \n",
      "    class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      "     |  DataFrame(jdf, sql_ctx)\n",
      "     |  \n",
      "     |  A distributed collection of data grouped into named columns.\n",
      "     |  \n",
      "     |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      "     |  and can be created using various functions in :class:`SparkSession`::\n",
      "     |  \n",
      "     |      people = spark.read.parquet(\"...\")\n",
      "     |  \n",
      "     |  Once created, it can be manipulated using the various domain-specific-language\n",
      "     |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      "     |  \n",
      "     |  To select a column from the :class:`DataFrame`, use the apply method::\n",
      "     |  \n",
      "     |      ageCol = people.age\n",
      "     |  \n",
      "     |  A more concrete example::\n",
      "     |  \n",
      "     |      # To create DataFrame using SparkSession\n",
      "     |      people = spark.read.parquet(\"...\")\n",
      "     |      department = spark.read.parquet(\"...\")\n",
      "     |  \n",
      "     |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      "     |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DataFrame\n",
      "     |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      "     |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |      Returns the :class:`Column` denoted by ``name``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.select(df.age).collect()\n",
      "     |      [Row(age=2), Row(age=5)]\n",
      "     |  \n",
      "     |  __getitem__(self, item)\n",
      "     |      Returns the column as a :class:`Column`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.select(df['age']).collect()\n",
      "     |      [Row(age=2), Row(age=5)]\n",
      "     |      >>> df[ [\"name\", \"age\"]].collect()\n",
      "     |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "     |      >>> df[ df.age > 3 ].collect()\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |      >>> df[df[0] > 3].collect()\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |  \n",
      "     |  __init__(self, jdf, sql_ctx)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  agg(self, *exprs)\n",
      "     |      Aggregate on the entire :class:`DataFrame` without groups\n",
      "     |      (shorthand for ``df.groupBy().agg()``).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      "     |      [Row(max(age)=5)]\n",
      "     |      >>> from pyspark.sql import functions as F\n",
      "     |      >>> df.agg(F.min(df.age)).collect()\n",
      "     |      [Row(min(age)=2)]\n",
      "     |  \n",
      "     |  alias(self, alias)\n",
      "     |      Returns a new :class:`DataFrame` with an alias set.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alias : str\n",
      "     |          an alias name to be set for the :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import *\n",
      "     |      >>> df_as1 = df.alias(\"df_as1\")\n",
      "     |      >>> df_as2 = df.alias(\"df_as2\")\n",
      "     |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      "     |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n",
      "     |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      "     |  \n",
      "     |  approxQuantile(self, col, probabilities, relativeError)\n",
      "     |      Calculates the approximate quantiles of numerical columns of a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The result of this algorithm has the following deterministic bound:\n",
      "     |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      "     |      probability `p` up to error `err`, then the algorithm will return\n",
      "     |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      "     |      close to (p * N). More precisely,\n",
      "     |      \n",
      "     |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      "     |      \n",
      "     |      This method implements a variation of the Greenwald-Khanna\n",
      "     |      algorithm (with some speed optimizations). The algorithm was first\n",
      "     |      present in [[https://doi.org/10.1145/375663.375670\n",
      "     |      Space-efficient Online Computation of Quantile Summaries]]\n",
      "     |      by Greenwald and Khanna.\n",
      "     |      \n",
      "     |      Note that null values will be ignored in numerical columns before calculation.\n",
      "     |      For columns only containing null values, an empty list is returned.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col: str, tuple or list\n",
      "     |          Can be a single column name, or a list of names for multiple columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 2.2\n",
      "     |             Added support for multiple columns.\n",
      "     |      probabilities : list or tuple\n",
      "     |          a list of quantile probabilities\n",
      "     |          Each number must belong to [0, 1].\n",
      "     |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      "     |      relativeError : float\n",
      "     |          The relative target precision to achieve\n",
      "     |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      "     |          could be very expensive. Note that values greater than 1 are\n",
      "     |          accepted but give the same result as 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the approximate quantiles at the given probabilities. If\n",
      "     |          the input `col` is a string, the output is a list of floats. If the\n",
      "     |          input `col` is a list or tuple of strings, the output is also a\n",
      "     |          list, but each element in it is a list of floats, i.e., the output\n",
      "     |          is a list of list of floats.\n",
      "     |  \n",
      "     |  cache(self)\n",
      "     |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      "     |  \n",
      "     |  checkpoint(self, eager=True)\n",
      "     |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n",
      "     |      logical plan of this :class:`DataFrame`, which is especially useful in iterative algorithms\n",
      "     |      where the plan may grow exponentially. It will be saved to files inside the checkpoint\n",
      "     |      directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eager : bool, optional\n",
      "     |          Whether to checkpoint this :class:`DataFrame` immediately\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental.\n",
      "     |  \n",
      "     |  coalesce(self, numPartitions)\n",
      "     |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      "     |      \n",
      "     |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      "     |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      "     |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      "     |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      "     |      it will stay at the current number of partitions.\n",
      "     |      \n",
      "     |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      "     |      this may result in your computation taking place on fewer nodes than\n",
      "     |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      "     |      you can call repartition(). This will add a shuffle step, but means the\n",
      "     |      current upstream partitions will be executed in parallel (per whatever\n",
      "     |      the current partitioning is).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          specify the target number of partitions\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      "     |      1\n",
      "     |  \n",
      "     |  colRegex(self, colName)\n",
      "     |      Selects column based on the column name specified as a regex and returns it\n",
      "     |      as :class:`Column`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colName : str\n",
      "     |          string, column name specified as a regex.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      "     |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      "     |      +----+\n",
      "     |      |Col2|\n",
      "     |      +----+\n",
      "     |      |   1|\n",
      "     |      |   2|\n",
      "     |      |   3|\n",
      "     |      +----+\n",
      "     |  \n",
      "     |  collect(self)\n",
      "     |      Returns all the records as a list of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.collect()\n",
      "     |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "     |  \n",
      "     |  corr(self, col1, col2, method=None)\n",
      "     |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      "     |      Currently only supports the Pearson Correlation Coefficient.\n",
      "     |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      method : str, optional\n",
      "     |          The correlation method. Currently only supports \"pearson\"\n",
      "     |  \n",
      "     |  count(self)\n",
      "     |      Returns the number of rows in this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.count()\n",
      "     |      2\n",
      "     |  \n",
      "     |  cov(self, col1, col2)\n",
      "     |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      "     |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |  \n",
      "     |  createGlobalTempView(self, name)\n",
      "     |      Creates a global temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary view is tied to this Spark application.\n",
      "     |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "     |      catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.createGlobalTempView(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      "     |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "     |  \n",
      "     |  createOrReplaceGlobalTempView(self, name)\n",
      "     |      Creates or replaces a global temporary view using the given name.\n",
      "     |      \n",
      "     |      The lifetime of this temporary view is tied to this Spark application.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      "     |      >>> df2 = df.filter(df.age > 3)\n",
      "     |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      "     |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      "     |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "     |  \n",
      "     |  createOrReplaceTempView(self, name)\n",
      "     |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.createOrReplaceTempView(\"people\")\n",
      "     |      >>> df2 = df.filter(df.age > 3)\n",
      "     |      >>> df2.createOrReplaceTempView(\"people\")\n",
      "     |      >>> df3 = spark.sql(\"select * from people\")\n",
      "     |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |  \n",
      "     |  createTempView(self, name)\n",
      "     |      Creates a local temporary view with this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "     |      catalog.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.createTempView(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"select * from people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "     |      Traceback (most recent call last):\n",
      "     |      ...\n",
      "     |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |  \n",
      "     |  crossJoin(self, other)\n",
      "     |      Returns the cartesian product with another :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Right side of the cartesian product.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.select(\"age\", \"name\").collect()\n",
      "     |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "     |      >>> df2.select(\"name\", \"height\").collect()\n",
      "     |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      "     |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      "     |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      "     |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      "     |  \n",
      "     |  crosstab(self, col1, col2)\n",
      "     |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      "     |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      "     |      non-zero pair frequencies will be returned.\n",
      "     |      The first column of each row will be the distinct values of `col1` and the column names\n",
      "     |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      "     |      Pairs that have no occurrences will have zero as their counts.\n",
      "     |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column. Distinct items will make the first item of\n",
      "     |          each row.\n",
      "     |      col2 : str\n",
      "     |          The name of the second column. Distinct items will make the column names\n",
      "     |          of the :class:`DataFrame`.\n",
      "     |  \n",
      "     |  cube(self, *cols)\n",
      "     |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      "     |      the specified columns, so we can run aggregations on them.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "     |      +-----+----+-----+\n",
      "     |      | name| age|count|\n",
      "     |      +-----+----+-----+\n",
      "     |      | null|null|    2|\n",
      "     |      | null|   2|    1|\n",
      "     |      | null|   5|    1|\n",
      "     |      |Alice|null|    1|\n",
      "     |      |Alice|   2|    1|\n",
      "     |      |  Bob|null|    1|\n",
      "     |      |  Bob|   5|    1|\n",
      "     |      +-----+----+-----+\n",
      "     |  \n",
      "     |  describe(self, *cols)\n",
      "     |      Computes basic statistics for numeric and string columns.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      This include count, mean, stddev, min, and max. If no columns are\n",
      "     |      given, this function computes statistics for all numerical or string columns.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Use summary for expanded statistics and control over which statistics to compute.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.describe(['age']).show()\n",
      "     |      +-------+------------------+\n",
      "     |      |summary|               age|\n",
      "     |      +-------+------------------+\n",
      "     |      |  count|                 2|\n",
      "     |      |   mean|               3.5|\n",
      "     |      | stddev|2.1213203435596424|\n",
      "     |      |    min|                 2|\n",
      "     |      |    max|                 5|\n",
      "     |      +-------+------------------+\n",
      "     |      >>> df.describe().show()\n",
      "     |      +-------+------------------+-----+\n",
      "     |      |summary|               age| name|\n",
      "     |      +-------+------------------+-----+\n",
      "     |      |  count|                 2|    2|\n",
      "     |      |   mean|               3.5| null|\n",
      "     |      | stddev|2.1213203435596424| null|\n",
      "     |      |    min|                 2|Alice|\n",
      "     |      |    max|                 5|  Bob|\n",
      "     |      +-------+------------------+-----+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.summary\n",
      "     |  \n",
      "     |  distinct(self)\n",
      "     |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.distinct().count()\n",
      "     |      2\n",
      "     |  \n",
      "     |  drop(self, *cols)\n",
      "     |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      "     |      This is a no-op if schema doesn't contain the given column name(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols: str or :class:`Column`\n",
      "     |          a name of the column, or the :class:`Column` to drop\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.drop('age').collect()\n",
      "     |      [Row(name='Alice'), Row(name='Bob')]\n",
      "     |      \n",
      "     |      >>> df.drop(df.age).collect()\n",
      "     |      [Row(name='Alice'), Row(name='Bob')]\n",
      "     |      \n",
      "     |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      "     |      [Row(age=5, height=85, name='Bob')]\n",
      "     |      \n",
      "     |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      "     |      [Row(age=5, name='Bob', height=85)]\n",
      "     |      \n",
      "     |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      "     |      [Row(name='Bob')]\n",
      "     |  \n",
      "     |  dropDuplicates(self, subset=None)\n",
      "     |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "     |      optionally only considering certain columns.\n",
      "     |      \n",
      "     |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "     |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "     |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "     |      be and system will accordingly limit the state. In addition, too late data older than\n",
      "     |      watermark will be dropped to avoid any possibility of duplicates.\n",
      "     |      \n",
      "     |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql import Row\n",
      "     |      >>> df = sc.parallelize([ \\\n",
      "     |      ...     Row(name='Alice', age=5, height=80), \\\n",
      "     |      ...     Row(name='Alice', age=5, height=80), \\\n",
      "     |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "     |      >>> df.dropDuplicates().show()\n",
      "     |      +-----+---+------+\n",
      "     |      | name|age|height|\n",
      "     |      +-----+---+------+\n",
      "     |      |Alice|  5|    80|\n",
      "     |      |Alice| 10|    80|\n",
      "     |      +-----+---+------+\n",
      "     |      \n",
      "     |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      "     |      +-----+---+------+\n",
      "     |      | name|age|height|\n",
      "     |      +-----+---+------+\n",
      "     |      |Alice|  5|    80|\n",
      "     |      +-----+---+------+\n",
      "     |  \n",
      "     |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      "     |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  dropna(self, how='any', thresh=None, subset=None)\n",
      "     |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "     |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      how : str, optional\n",
      "     |          'any' or 'all'.\n",
      "     |          If 'any', drop a row if it contains any nulls.\n",
      "     |          If 'all', drop a row only if all its values are null.\n",
      "     |      thresh: int, optional\n",
      "     |          default None\n",
      "     |          If specified, drop rows that have less than `thresh` non-null values.\n",
      "     |          This overwrites the `how` parameter.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df4.na.drop().show()\n",
      "     |      +---+------+-----+\n",
      "     |      |age|height| name|\n",
      "     |      +---+------+-----+\n",
      "     |      | 10|    80|Alice|\n",
      "     |      +---+------+-----+\n",
      "     |  \n",
      "     |  exceptAll(self, other)\n",
      "     |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      "     |      not in another :class:`DataFrame` while preserving duplicates.\n",
      "     |      \n",
      "     |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      "     |      As standard in SQL, this function resolves columns by position (not by name).\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame(\n",
      "     |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      \n",
      "     |      >>> df1.exceptAll(df2).show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  2|\n",
      "     |      |  c|  4|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  explain(self, extended=None, mode=None)\n",
      "     |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      parameters\n",
      "     |      ----------\n",
      "     |      extended : bool, optional\n",
      "     |          default ``False``. If ``False``, prints only the physical plan.\n",
      "     |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      "     |          specified.\n",
      "     |      mode : str, optional\n",
      "     |          specifies the expected output format of plans.\n",
      "     |      \n",
      "     |          * ``simple``: Print only a physical plan.\n",
      "     |          * ``extended``: Print both logical and physical plans.\n",
      "     |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      "     |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      "     |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0.0\n",
      "     |             Added optional argument `mode` to specify the expected output format of plans.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.explain()\n",
      "     |      == Physical Plan ==\n",
      "     |      *(1) Scan ExistingRDD[age#0,name#1]\n",
      "     |      \n",
      "     |      >>> df.explain(True)\n",
      "     |      == Parsed Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Analyzed Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Optimized Logical Plan ==\n",
      "     |      ...\n",
      "     |      == Physical Plan ==\n",
      "     |      ...\n",
      "     |      \n",
      "     |      >>> df.explain(mode=\"formatted\")\n",
      "     |      == Physical Plan ==\n",
      "     |      * Scan ExistingRDD (1)\n",
      "     |      (1) Scan ExistingRDD [codegen id : 1]\n",
      "     |      Output [2]: [age#0, name#1]\n",
      "     |      ...\n",
      "     |      \n",
      "     |      >>> df.explain(\"cost\")\n",
      "     |      == Optimized Logical Plan ==\n",
      "     |      ...Statistics...\n",
      "     |      ...\n",
      "     |  \n",
      "     |  fillna(self, value, subset=None)\n",
      "     |      Replace null values, alias for ``na.fill()``.\n",
      "     |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : int, float, string, bool or dict\n",
      "     |          Value to replace null values with.\n",
      "     |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      "     |          from column name (string) to replacement value. The replacement value must be\n",
      "     |          an int, float, boolean, or string.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data type are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df4.na.fill(50).show()\n",
      "     |      +---+------+-----+\n",
      "     |      |age|height| name|\n",
      "     |      +---+------+-----+\n",
      "     |      | 10|    80|Alice|\n",
      "     |      |  5|    50|  Bob|\n",
      "     |      | 50|    50|  Tom|\n",
      "     |      | 50|    50| null|\n",
      "     |      +---+------+-----+\n",
      "     |      \n",
      "     |      >>> df5.na.fill(False).show()\n",
      "     |      +----+-------+-----+\n",
      "     |      | age|   name|  spy|\n",
      "     |      +----+-------+-----+\n",
      "     |      |  10|  Alice|false|\n",
      "     |      |   5|    Bob|false|\n",
      "     |      |null|Mallory| true|\n",
      "     |      +----+-------+-----+\n",
      "     |      \n",
      "     |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      "     |      +---+------+-------+\n",
      "     |      |age|height|   name|\n",
      "     |      +---+------+-------+\n",
      "     |      | 10|    80|  Alice|\n",
      "     |      |  5|  null|    Bob|\n",
      "     |      | 50|  null|    Tom|\n",
      "     |      | 50|  null|unknown|\n",
      "     |      +---+------+-------+\n",
      "     |  \n",
      "     |  filter(self, condition)\n",
      "     |      Filters rows using the given condition.\n",
      "     |      \n",
      "     |      :func:`where` is an alias for :func:`filter`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      condition : :class:`Column` or str\n",
      "     |          a :class:`Column` of :class:`types.BooleanType`\n",
      "     |          or a string of SQL expression.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.filter(df.age > 3).collect()\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |      >>> df.where(df.age == 2).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |      \n",
      "     |      >>> df.filter(\"age > 3\").collect()\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |      >>> df.where(\"age = 2\").collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  first(self)\n",
      "     |      Returns the first row as a :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.first()\n",
      "     |      Row(age=2, name='Alice')\n",
      "     |  \n",
      "     |  foreach(self, f)\n",
      "     |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is a shorthand for ``df.rdd.foreach()``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> def f(person):\n",
      "     |      ...     print(person.name)\n",
      "     |      >>> df.foreach(f)\n",
      "     |  \n",
      "     |  foreachPartition(self, f)\n",
      "     |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> def f(people):\n",
      "     |      ...     for person in people:\n",
      "     |      ...         print(person.name)\n",
      "     |      >>> df.foreachPartition(f)\n",
      "     |  \n",
      "     |  freqItems(self, cols, support=None)\n",
      "     |      Finding frequent items for columns, possibly with false positives. Using the\n",
      "     |      frequent element count algorithm described in\n",
      "     |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      "     |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list or tuple\n",
      "     |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      "     |          strings.\n",
      "     |      support : float, optional\n",
      "     |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      "     |          The support must be greater than 1e-4.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |  \n",
      "     |  groupBy(self, *cols)\n",
      "     |      Groups the :class:`DataFrame` using the specified columns,\n",
      "     |      so we can run aggregation on them. See :class:`GroupedData`\n",
      "     |      for all the available aggregate functions.\n",
      "     |      \n",
      "     |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list, str or :class:`Column`\n",
      "     |          columns to group by.\n",
      "     |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.groupBy().avg().collect()\n",
      "     |      [Row(avg(age)=3.5)]\n",
      "     |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      "     |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "     |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      "     |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "     |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      "     |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      "     |  \n",
      "     |  groupby = groupBy(self, *cols)\n",
      "     |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  head(self, n=None)\n",
      "     |      Returns the first ``n`` rows.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting array is expected\n",
      "     |      to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, optional\n",
      "     |          default 1. Number of rows to return.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      If n is greater than 1, return a list of :class:`Row`.\n",
      "     |      If n is 1, return a single Row.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.head()\n",
      "     |      Row(age=2, name='Alice')\n",
      "     |      >>> df.head(1)\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  hint(self, name, *parameters)\n",
      "     |      Specifies some hint on the current :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.2.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      name : str\n",
      "     |          A name of the hint.\n",
      "     |      parameters : str, list, float or int\n",
      "     |          Optional parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrame`\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      "     |      +----+---+------+\n",
      "     |      |name|age|height|\n",
      "     |      +----+---+------+\n",
      "     |      | Bob|  5|    85|\n",
      "     |      +----+---+------+\n",
      "     |  \n",
      "     |  inputFiles(self)\n",
      "     |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      "     |      This method simply asks each constituent BaseRelation for its respective files and\n",
      "     |      takes the union of all results. Depending on the source relations, this may not find\n",
      "     |      all input files. Duplicates are removed.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
      "     |      >>> len(df.inputFiles())\n",
      "     |      1\n",
      "     |  \n",
      "     |  intersect(self, other)\n",
      "     |      Return a new :class:`DataFrame` containing rows only in\n",
      "     |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is equivalent to `INTERSECT` in SQL.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  intersectAll(self, other)\n",
      "     |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      "     |      and another :class:`DataFrame` while preserving duplicates.\n",
      "     |      \n",
      "     |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      "     |      resolves columns by position (not by name).\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "     |      \n",
      "     |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      "     |      +---+---+\n",
      "     |      | C1| C2|\n",
      "     |      +---+---+\n",
      "     |      |  a|  1|\n",
      "     |      |  a|  1|\n",
      "     |      |  b|  3|\n",
      "     |      +---+---+\n",
      "     |  \n",
      "     |  isLocal(self)\n",
      "     |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      "     |      (without any Spark executors).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  join(self, other, on=None, how=None)\n",
      "     |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : :class:`DataFrame`\n",
      "     |          Right side of the join\n",
      "     |      on : str, list or :class:`Column`, optional\n",
      "     |          a string for the join column name, a list of column names,\n",
      "     |          a join expression (Column), or a list of Columns.\n",
      "     |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "     |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      "     |      how : str, optional\n",
      "     |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "     |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "     |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "     |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import desc\n",
      "     |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      "     |      [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      "     |      \n",
      "     |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      "     |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "     |      \n",
      "     |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      "     |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      "     |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "     |      \n",
      "     |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      "     |      [Row(name='Bob', height=85)]\n",
      "     |      \n",
      "     |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      "     |      [Row(name='Bob', age=5)]\n",
      "     |  \n",
      "     |  limit(self, num)\n",
      "     |      Limits the result count to the number specified.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.limit(1).collect()\n",
      "     |      [Row(age=2, name='Alice')]\n",
      "     |      >>> df.limit(0).collect()\n",
      "     |      []\n",
      "     |  \n",
      "     |  localCheckpoint(self, eager=True)\n",
      "     |      Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n",
      "     |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "     |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      "     |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eager : bool, optional\n",
      "     |          Whether to checkpoint this :class:`DataFrame` immediately\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental.\n",
      "     |  \n",
      "     |  orderBy = sort(self, *cols, **kwargs)\n",
      "     |  \n",
      "     |  persist(self, storageLevel=StorageLevel(True, True, False, True, 1))\n",
      "     |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      "     |      operations after the first time it is computed. This can only be used to assign\n",
      "     |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      "     |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      "     |  \n",
      "     |  printSchema(self)\n",
      "     |      Prints out the schema in the tree format.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.printSchema()\n",
      "     |      root\n",
      "     |       |-- age: integer (nullable = true)\n",
      "     |       |-- name: string (nullable = true)\n",
      "     |      <BLANKLINE>\n",
      "     |  \n",
      "     |  randomSplit(self, weights, seed=None)\n",
      "     |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weights : list\n",
      "     |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
      "     |          Weights will be normalized if they don't sum up to 1.0.\n",
      "     |      seed : int, optional\n",
      "     |          The seed for sampling.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      "     |      >>> splits[0].count()\n",
      "     |      2\n",
      "     |      \n",
      "     |      >>> splits[1].count()\n",
      "     |      2\n",
      "     |  \n",
      "     |  registerTempTable(self, name)\n",
      "     |      Registers this DataFrame as a temporary table using the given name.\n",
      "     |      \n",
      "     |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "     |      that was used to create this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      .. deprecated:: 2.0.0\n",
      "     |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.registerTempTable(\"people\")\n",
      "     |      >>> df2 = spark.sql(\"select * from people\")\n",
      "     |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "     |      True\n",
      "     |      >>> spark.catalog.dropTempView(\"people\")\n",
      "     |  \n",
      "     |  repartition(self, numPartitions, *cols)\n",
      "     |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "     |      resulting :class:`DataFrame` is hash partitioned.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          can be an int to specify the target number of partitions or a Column.\n",
      "     |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "     |          the default number of partitions is used.\n",
      "     |      cols : str or :class:`Column`\n",
      "     |          partitioning columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 1.6\n",
      "     |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      "     |             optional if partitioning columns are specified.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      "     |      10\n",
      "     |      >>> data = df.union(df).repartition(\"age\")\n",
      "     |      >>> data.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |      >>> data = data.repartition(7, \"age\")\n",
      "     |      >>> data.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      >>> data.rdd.getNumPartitions()\n",
      "     |      7\n",
      "     |      >>> data = data.repartition(\"name\", \"age\")\n",
      "     |      >>> data.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  5|  Bob|\n",
      "     |      |  5|  Bob|\n",
      "     |      |  2|Alice|\n",
      "     |      |  2|Alice|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  repartitionByRange(self, numPartitions, *cols)\n",
      "     |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "     |      resulting :class:`DataFrame` is range partitioned.\n",
      "     |      \n",
      "     |      At least one partition-by expression must be specified.\n",
      "     |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      numPartitions : int\n",
      "     |          can be an int to specify the target number of partitions or a Column.\n",
      "     |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "     |          the default number of partitions is used.\n",
      "     |      cols : str or :class:`Column`\n",
      "     |          partitioning columns.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
      "     |      Hence, the output may not be consistent, since sampling can return different values.\n",
      "     |      The sample size can be controlled by the config\n",
      "     |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      "     |      2\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      "     |      1\n",
      "     |      >>> data = df.repartitionByRange(\"age\")\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  replace(self, to_replace, value=<no value>, subset=None)\n",
      "     |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      "     |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      "     |      aliases of each other.\n",
      "     |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      "     |      or strings. Value can have None. When replacing, the new value will be cast\n",
      "     |      to the type of the existing column.\n",
      "     |      For numeric replacements all values to be replaced should have unique\n",
      "     |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      "     |      and arbitrary replacement will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      to_replace : bool, int, float, string, list or dict\n",
      "     |          Value to be replaced.\n",
      "     |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      "     |          must be a mapping between a value and a replacement.\n",
      "     |      value : bool, int, float, string or None, optional\n",
      "     |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      "     |          list, `value` should be of the same length and type as `to_replace`.\n",
      "     |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      "     |          used as a replacement for each item in `to_replace`.\n",
      "     |      subset : list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data type are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df4.na.replace(10, 20).show()\n",
      "     |      +----+------+-----+\n",
      "     |      | age|height| name|\n",
      "     |      +----+------+-----+\n",
      "     |      |  20|    80|Alice|\n",
      "     |      |   5|  null|  Bob|\n",
      "     |      |null|  null|  Tom|\n",
      "     |      |null|  null| null|\n",
      "     |      +----+------+-----+\n",
      "     |      \n",
      "     |      >>> df4.na.replace('Alice', None).show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|null|\n",
      "     |      |   5|  null| Bob|\n",
      "     |      |null|  null| Tom|\n",
      "     |      |null|  null|null|\n",
      "     |      +----+------+----+\n",
      "     |      \n",
      "     |      >>> df4.na.replace({'Alice': None}).show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|null|\n",
      "     |      |   5|  null| Bob|\n",
      "     |      |null|  null| Tom|\n",
      "     |      |null|  null|null|\n",
      "     |      +----+------+----+\n",
      "     |      \n",
      "     |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|   A|\n",
      "     |      |   5|  null|   B|\n",
      "     |      |null|  null| Tom|\n",
      "     |      |null|  null|null|\n",
      "     |      +----+------+----+\n",
      "     |  \n",
      "     |  rollup(self, *cols)\n",
      "     |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      "     |      the specified columns, so we can run aggregation on them.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "     |      +-----+----+-----+\n",
      "     |      | name| age|count|\n",
      "     |      +-----+----+-----+\n",
      "     |      | null|null|    2|\n",
      "     |      |Alice|null|    1|\n",
      "     |      |Alice|   2|    1|\n",
      "     |      |  Bob|null|    1|\n",
      "     |      |  Bob|   5|    1|\n",
      "     |      +-----+----+-----+\n",
      "     |  \n",
      "     |  sameSemantics(self, other)\n",
      "     |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
      "     |      therefore return same results.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
      "     |      such as attribute names.\n",
      "     |      \n",
      "     |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
      "     |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      "     |      different plans. Such false negative semantic can be useful when caching as an example.\n",
      "     |      \n",
      "     |      This API is a developer API.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df1 = spark.range(10)\n",
      "     |      >>> df2 = spark.range(10)\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      "     |      True\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      "     |      False\n",
      "     |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      "     |      True\n",
      "     |  \n",
      "     |  sample(self, withReplacement=None, fraction=None, seed=None)\n",
      "     |      Returns a sampled subset of this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      withReplacement : bool, optional\n",
      "     |          Sample with replacement or not (default ``False``).\n",
      "     |      fraction : float, optional\n",
      "     |          Fraction of rows to generate, range [0.0, 1.0].\n",
      "     |      seed : int, optional\n",
      "     |          Seed for sampling (default a random seed).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      "     |      count of the given :class:`DataFrame`.\n",
      "     |      \n",
      "     |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df = spark.range(10)\n",
      "     |      >>> df.sample(0.5, 3).count()\n",
      "     |      7\n",
      "     |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      "     |      7\n",
      "     |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      "     |      1\n",
      "     |      >>> df.sample(1.0).count()\n",
      "     |      10\n",
      "     |      >>> df.sample(fraction=1.0).count()\n",
      "     |      10\n",
      "     |      >>> df.sample(False, fraction=1.0).count()\n",
      "     |      10\n",
      "     |  \n",
      "     |  sampleBy(self, col, fractions, seed=None)\n",
      "     |      Returns a stratified sample without replacement based on the\n",
      "     |      fraction given on each stratum.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col : :class:`Column` or str\n",
      "     |          column that defines strata\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0\n",
      "     |             Added sampling by a column of :class:`Column`\n",
      "     |      fractions : dict\n",
      "     |          sampling fraction for each stratum. If a stratum is not\n",
      "     |          specified, we treat its fraction as zero.\n",
      "     |      seed : int, optional\n",
      "     |          random seed\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a new :class:`DataFrame` that represents the stratified sample\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      "     |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      "     |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      "     |      +---+-----+\n",
      "     |      |key|count|\n",
      "     |      +---+-----+\n",
      "     |      |  0|    3|\n",
      "     |      |  1|    6|\n",
      "     |      +---+-----+\n",
      "     |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      "     |      33\n",
      "     |  \n",
      "     |  select(self, *cols)\n",
      "     |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, :class:`Column`, or list\n",
      "     |          column names (string) or expressions (:class:`Column`).\n",
      "     |          If one of the column names is '*', that column is expanded to include all columns\n",
      "     |          in the current :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.select('*').collect()\n",
      "     |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "     |      >>> df.select('name', 'age').collect()\n",
      "     |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "     |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      "     |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      "     |  \n",
      "     |  selectExpr(self, *expr)\n",
      "     |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      "     |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      "     |  \n",
      "     |  semanticHash(self)\n",
      "     |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Unlike the standard hash code, the hash is calculated against the query plan\n",
      "     |      simplified by tolerating the cosmetic differences such as attribute names.\n",
      "     |      \n",
      "     |      This API is a developer API.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      "     |      1855039936\n",
      "     |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      "     |      1855039936\n",
      "     |  \n",
      "     |  show(self, n=20, truncate=True, vertical=False)\n",
      "     |      Prints the first ``n`` rows to the console.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, optional\n",
      "     |          Number of rows to show.\n",
      "     |      truncate : bool, optional\n",
      "     |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
      "     |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      "     |          and align cells right.\n",
      "     |      vertical : bool, optional\n",
      "     |          If set to ``True``, print output rows vertically (one line\n",
      "     |          per column value).\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df\n",
      "     |      DataFrame[age: int, name: string]\n",
      "     |      >>> df.show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |      >>> df.show(truncate=3)\n",
      "     |      +---+----+\n",
      "     |      |age|name|\n",
      "     |      +---+----+\n",
      "     |      |  2| Ali|\n",
      "     |      |  5| Bob|\n",
      "     |      +---+----+\n",
      "     |      >>> df.show(vertical=True)\n",
      "     |      -RECORD 0-----\n",
      "     |       age  | 2\n",
      "     |       name | Alice\n",
      "     |      -RECORD 1-----\n",
      "     |       age  | 5\n",
      "     |       name | Bob\n",
      "     |  \n",
      "     |  sort(self, *cols, **kwargs)\n",
      "     |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, list, or :class:`Column`, optional\n",
      "     |           list of :class:`Column` or column names to sort by.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      ascending : bool or list, optional\n",
      "     |          boolean or list of boolean (default ``True``).\n",
      "     |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "     |          If a list is specified, length of the list must equal length of the `cols`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.sort(df.age.desc()).collect()\n",
      "     |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "     |      >>> df.sort(\"age\", ascending=False).collect()\n",
      "     |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "     |      >>> df.orderBy(df.age.desc()).collect()\n",
      "     |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "     |      >>> from pyspark.sql.functions import *\n",
      "     |      >>> df.sort(asc(\"age\")).collect()\n",
      "     |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "     |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      "     |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "     |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      "     |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "     |  \n",
      "     |  sortWithinPartitions(self, *cols, **kwargs)\n",
      "     |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str, list or :class:`Column`, optional\n",
      "     |          list of :class:`Column` or column names to sort by.\n",
      "     |      \n",
      "     |      Other Parameters\n",
      "     |      ----------------\n",
      "     |      ascending : bool or list, optional\n",
      "     |          boolean or list of boolean (default ``True``).\n",
      "     |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "     |          If a list is specified, length of the list must equal length of the `cols`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      "     |      +---+-----+\n",
      "     |      |age| name|\n",
      "     |      +---+-----+\n",
      "     |      |  2|Alice|\n",
      "     |      |  5|  Bob|\n",
      "     |      +---+-----+\n",
      "     |  \n",
      "     |  subtract(self, other)\n",
      "     |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      "     |      but not in another :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  summary(self, *statistics)\n",
      "     |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      "     |      - count\n",
      "     |      - mean\n",
      "     |      - stddev\n",
      "     |      - min\n",
      "     |      - max\n",
      "     |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      "     |      \n",
      "     |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      "     |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.summary().show()\n",
      "     |      +-------+------------------+-----+\n",
      "     |      |summary|               age| name|\n",
      "     |      +-------+------------------+-----+\n",
      "     |      |  count|                 2|    2|\n",
      "     |      |   mean|               3.5| null|\n",
      "     |      | stddev|2.1213203435596424| null|\n",
      "     |      |    min|                 2|Alice|\n",
      "     |      |    25%|                 2| null|\n",
      "     |      |    50%|                 2| null|\n",
      "     |      |    75%|                 5| null|\n",
      "     |      |    max|                 5|  Bob|\n",
      "     |      +-------+------------------+-----+\n",
      "     |      \n",
      "     |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      "     |      +-------+---+-----+\n",
      "     |      |summary|age| name|\n",
      "     |      +-------+---+-----+\n",
      "     |      |  count|  2|    2|\n",
      "     |      |    min|  2|Alice|\n",
      "     |      |    25%|  2| null|\n",
      "     |      |    75%|  5| null|\n",
      "     |      |    max|  5|  Bob|\n",
      "     |      +-------+---+-----+\n",
      "     |      \n",
      "     |      To do a summary for specific columns first select them:\n",
      "     |      \n",
      "     |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n",
      "     |      +-------+---+----+\n",
      "     |      |summary|age|name|\n",
      "     |      +-------+---+----+\n",
      "     |      |  count|  2|   2|\n",
      "     |      +-------+---+----+\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      DataFrame.display\n",
      "     |  \n",
      "     |  tail(self, num)\n",
      "     |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "     |      \n",
      "     |      Running tail requires moving data into the application's driver process, and doing so with\n",
      "     |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.tail(1)\n",
      "     |      [Row(age=5, name='Bob')]\n",
      "     |  \n",
      "     |  take(self, num)\n",
      "     |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.take(2)\n",
      "     |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "     |  \n",
      "     |  toDF(self, *cols)\n",
      "     |      Returns a new :class:`DataFrame` that with new specified column names\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : str\n",
      "     |          new column names\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.toDF('f1', 'f2').collect()\n",
      "     |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      "     |  \n",
      "     |  toJSON(self, use_unicode=True)\n",
      "     |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      "     |      \n",
      "     |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.toJSON().first()\n",
      "     |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      "     |  \n",
      "     |  toLocalIterator(self, prefetchPartitions=False)\n",
      "     |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      "     |      The iterator will consume as much memory as the largest partition in this\n",
      "     |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      "     |      partitions.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      prefetchPartitions : bool, optional\n",
      "     |          If Spark should pre-fetch the next partition  before it is needed.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> list(df.toLocalIterator())\n",
      "     |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "     |  \n",
      "     |  transform(self, func)\n",
      "     |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a function that takes and returns a :class:`DataFrame`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      "     |      >>> def cast_all_to_int(input_df):\n",
      "     |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      "     |      >>> def sort_columns_asc(input_df):\n",
      "     |      ...     return input_df.select(*sorted(input_df.columns))\n",
      "     |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      "     |      +-----+---+\n",
      "     |      |float|int|\n",
      "     |      +-----+---+\n",
      "     |      |    1|  1|\n",
      "     |      |    2|  2|\n",
      "     |      +-----+---+\n",
      "     |  \n",
      "     |  union(self, other)\n",
      "     |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      "     |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "     |      \n",
      "     |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0\n",
      "     |  \n",
      "     |  unionAll(self, other)\n",
      "     |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      "     |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "     |      \n",
      "     |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  unionByName(self, other, allowMissingColumns=False)\n",
      "     |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      "     |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      The difference between this function and :func:`union` is that this function\n",
      "     |      resolves columns by name (not by position):\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      "     |      >>> df1.unionByName(df2).show()\n",
      "     |      +----+----+----+\n",
      "     |      |col0|col1|col2|\n",
      "     |      +----+----+----+\n",
      "     |      |   1|   2|   3|\n",
      "     |      |   6|   4|   5|\n",
      "     |      +----+----+----+\n",
      "     |      \n",
      "     |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n",
      "     |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n",
      "     |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n",
      "     |      in the schema of the union result:\n",
      "     |      \n",
      "     |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "     |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      "     |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "     |      +----+----+----+----+\n",
      "     |      |col0|col1|col2|col3|\n",
      "     |      +----+----+----+----+\n",
      "     |      |   1|   2|   3|null|\n",
      "     |      |null|   4|   5|   6|\n",
      "     |      +----+----+----+----+\n",
      "     |      \n",
      "     |      .. versionchanged:: 3.1.0\n",
      "     |         Added optional argument `allowMissingColumns` to specify whether to allow\n",
      "     |         missing columns.\n",
      "     |  \n",
      "     |  unpersist(self, blocking=False)\n",
      "     |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      "     |      memory and disk.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      "     |  \n",
      "     |  where = filter(self, condition)\n",
      "     |      :func:`where` is an alias for :func:`filter`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  withColumn(self, colName, col)\n",
      "     |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "     |      existing column that has the same name.\n",
      "     |      \n",
      "     |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "     |      a column from some other :class:`DataFrame` will raise an error.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      colName : str\n",
      "     |          string, name of the new column.\n",
      "     |      col : :class:`Column`\n",
      "     |          a :class:`Column` expression for the new column.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method introduces a projection internally. Therefore, calling it multiple\n",
      "     |      times, for instance, via loops in order to add multiple columns can generate big\n",
      "     |      plans which can cause performance issues and even `StackOverflowException`.\n",
      "     |      To avoid this, use :func:`select` with the multiple columns at once.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      "     |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      "     |  \n",
      "     |  withColumnRenamed(self, existing, new)\n",
      "     |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      "     |      This is a no-op if schema doesn't contain the given column name.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      existing : str\n",
      "     |          string, name of the existing column to rename.\n",
      "     |      new : str\n",
      "     |          string, new name of the column.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      "     |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      "     |  \n",
      "     |  withWatermark(self, eventTime, delayThreshold)\n",
      "     |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      "     |      in time before which we assume no more late data is going to arrive.\n",
      "     |      \n",
      "     |      Spark will use this watermark for several purposes:\n",
      "     |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      "     |          when using output modes that do not allow updates.\n",
      "     |      \n",
      "     |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      "     |      \n",
      "     |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      "     |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      "     |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      "     |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      "     |      process records that arrive more than `delayThreshold` late.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      eventTime : str\n",
      "     |          the name of the column that contains the event time of the row.\n",
      "     |      delayThreshold : str\n",
      "     |          the minimum delay to wait to data to arrive late, relative to the\n",
      "     |          latest record that has been processed in the form of an interval\n",
      "     |          (e.g. \"1 minute\" or \"5 hours\").\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      >>> from pyspark.sql.functions import timestamp_seconds\n",
      "     |      >>> sdf.select(\n",
      "     |      ...    'name',\n",
      "     |      ...    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n",
      "     |      DataFrame[name: string, time: timestamp]\n",
      "     |  \n",
      "     |  writeTo(self, table)\n",
      "     |      Create a write configuration builder for v2 sources.\n",
      "     |      \n",
      "     |      This builder is used to configure and execute write operations.\n",
      "     |      \n",
      "     |      For example, to append or create or replace existing tables.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.1.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      "     |      >>> df.writeTo(                              # doctest: +SKIP\n",
      "     |      ...     \"catalog.db.table\"\n",
      "     |      ... ).partitionedBy(\"col\").createOrReplace()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  columns\n",
      "     |      Returns all column names as a list.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.columns\n",
      "     |      ['age', 'name']\n",
      "     |  \n",
      "     |  dtypes\n",
      "     |      Returns all column names and their data types as a list.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.dtypes\n",
      "     |      [('age', 'int'), ('name', 'string')]\n",
      "     |  \n",
      "     |  isStreaming\n",
      "     |      Returns ``True`` if this :class:`Dataset` contains one or more sources that continuously\n",
      "     |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n",
      "     |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n",
      "     |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n",
      "     |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n",
      "     |      source present.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |  \n",
      "     |  na\n",
      "     |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |  \n",
      "     |  rdd\n",
      "     |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  schema\n",
      "     |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.schema\n",
      "     |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n",
      "     |  \n",
      "     |  stat\n",
      "     |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  storageLevel\n",
      "     |      Get the :class:`DataFrame`'s current storage level.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.1.0\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.storageLevel\n",
      "     |      StorageLevel(False, False, False, False, 1)\n",
      "     |      >>> df.cache().storageLevel\n",
      "     |      StorageLevel(True, True, False, True, 1)\n",
      "     |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      "     |      StorageLevel(True, False, False, False, 2)\n",
      "     |  \n",
      "     |  write\n",
      "     |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      "     |      storage.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataFrameWriter`\n",
      "     |  \n",
      "     |  writeStream\n",
      "     |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      "     |      storage.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is evolving.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`DataStreamWriter`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      "     |  \n",
      "     |  mapInPandas(self, func, schema)\n",
      "     |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "     |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      "     |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      "     |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      "     |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      "     |      Each `pandas.DataFrame` size can be controlled by\n",
      "     |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      "     |      \n",
      "     |      .. versionadded:: 3.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : function\n",
      "     |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      "     |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      "     |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      "     |          the return type of the `func` in PySpark. The value can be either a\n",
      "     |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import pandas_udf\n",
      "     |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "     |      >>> def filter_func(iterator):\n",
      "     |      ...     for pdf in iterator:\n",
      "     |      ...         yield pdf[pdf.id == 1]\n",
      "     |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "     |      +---+---+\n",
      "     |      | id|age|\n",
      "     |      +---+---+\n",
      "     |      |  1| 21|\n",
      "     |      +---+---+\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This API is experimental\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      pyspark.sql.functions.pandas_udf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      "     |  \n",
      "     |  toPandas(self)\n",
      "     |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      "     |      \n",
      "     |      This is only available if Pandas is installed and available.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This method should only be used if the resulting Pandas's :class:`DataFrame` is\n",
      "     |      expected to be small, as all the data is loaded into the driver's memory.\n",
      "     |      \n",
      "     |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df.toPandas()  # doctest: +SKIP\n",
      "     |         age   name\n",
      "     |      0    2  Alice\n",
      "     |      1    5    Bob\n",
      "    \n",
      "    class DataFrameNaFunctions(builtins.object)\n",
      "     |  DataFrameNaFunctions(df)\n",
      "     |  \n",
      "     |  Functionality for working with missing data in :class:`DataFrame`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  drop(self, how='any', thresh=None, subset=None)\n",
      "     |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "     |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      how : str, optional\n",
      "     |          'any' or 'all'.\n",
      "     |          If 'any', drop a row if it contains any nulls.\n",
      "     |          If 'all', drop a row only if all its values are null.\n",
      "     |      thresh: int, optional\n",
      "     |          default None\n",
      "     |          If specified, drop rows that have less than `thresh` non-null values.\n",
      "     |          This overwrites the `how` parameter.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df4.na.drop().show()\n",
      "     |      +---+------+-----+\n",
      "     |      |age|height| name|\n",
      "     |      +---+------+-----+\n",
      "     |      | 10|    80|Alice|\n",
      "     |      +---+------+-----+\n",
      "     |  \n",
      "     |  fill(self, value, subset=None)\n",
      "     |      Replace null values, alias for ``na.fill()``.\n",
      "     |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.1\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      value : int, float, string, bool or dict\n",
      "     |          Value to replace null values with.\n",
      "     |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      "     |          from column name (string) to replacement value. The replacement value must be\n",
      "     |          an int, float, boolean, or string.\n",
      "     |      subset : str, tuple or list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data type are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df4.na.fill(50).show()\n",
      "     |      +---+------+-----+\n",
      "     |      |age|height| name|\n",
      "     |      +---+------+-----+\n",
      "     |      | 10|    80|Alice|\n",
      "     |      |  5|    50|  Bob|\n",
      "     |      | 50|    50|  Tom|\n",
      "     |      | 50|    50| null|\n",
      "     |      +---+------+-----+\n",
      "     |      \n",
      "     |      >>> df5.na.fill(False).show()\n",
      "     |      +----+-------+-----+\n",
      "     |      | age|   name|  spy|\n",
      "     |      +----+-------+-----+\n",
      "     |      |  10|  Alice|false|\n",
      "     |      |   5|    Bob|false|\n",
      "     |      |null|Mallory| true|\n",
      "     |      +----+-------+-----+\n",
      "     |      \n",
      "     |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      "     |      +---+------+-------+\n",
      "     |      |age|height|   name|\n",
      "     |      +---+------+-------+\n",
      "     |      | 10|    80|  Alice|\n",
      "     |      |  5|  null|    Bob|\n",
      "     |      | 50|  null|    Tom|\n",
      "     |      | 50|  null|unknown|\n",
      "     |      +---+------+-------+\n",
      "     |  \n",
      "     |  replace(self, to_replace, value=<no value>, subset=None)\n",
      "     |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      "     |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      "     |      aliases of each other.\n",
      "     |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      "     |      or strings. Value can have None. When replacing, the new value will be cast\n",
      "     |      to the type of the existing column.\n",
      "     |      For numeric replacements all values to be replaced should have unique\n",
      "     |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      "     |      and arbitrary replacement will be used.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      to_replace : bool, int, float, string, list or dict\n",
      "     |          Value to be replaced.\n",
      "     |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      "     |          must be a mapping between a value and a replacement.\n",
      "     |      value : bool, int, float, string or None, optional\n",
      "     |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      "     |          list, `value` should be of the same length and type as `to_replace`.\n",
      "     |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      "     |          used as a replacement for each item in `to_replace`.\n",
      "     |      subset : list, optional\n",
      "     |          optional list of column names to consider.\n",
      "     |          Columns specified in subset that do not have matching data type are ignored.\n",
      "     |          For example, if `value` is a string, and subset contains a non-string column,\n",
      "     |          then the non-string column is simply ignored.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> df4.na.replace(10, 20).show()\n",
      "     |      +----+------+-----+\n",
      "     |      | age|height| name|\n",
      "     |      +----+------+-----+\n",
      "     |      |  20|    80|Alice|\n",
      "     |      |   5|  null|  Bob|\n",
      "     |      |null|  null|  Tom|\n",
      "     |      |null|  null| null|\n",
      "     |      +----+------+-----+\n",
      "     |      \n",
      "     |      >>> df4.na.replace('Alice', None).show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|null|\n",
      "     |      |   5|  null| Bob|\n",
      "     |      |null|  null| Tom|\n",
      "     |      |null|  null|null|\n",
      "     |      +----+------+----+\n",
      "     |      \n",
      "     |      >>> df4.na.replace({'Alice': None}).show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|null|\n",
      "     |      |   5|  null| Bob|\n",
      "     |      |null|  null| Tom|\n",
      "     |      |null|  null|null|\n",
      "     |      +----+------+----+\n",
      "     |      \n",
      "     |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      "     |      +----+------+----+\n",
      "     |      | age|height|name|\n",
      "     |      +----+------+----+\n",
      "     |      |  10|    80|   A|\n",
      "     |      |   5|  null|   B|\n",
      "     |      |null|  null| Tom|\n",
      "     |      |null|  null|null|\n",
      "     |      +----+------+----+\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataFrameStatFunctions(builtins.object)\n",
      "     |  DataFrameStatFunctions(df)\n",
      "     |  \n",
      "     |  Functionality for statistic functions with :class:`DataFrame`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  approxQuantile(self, col, probabilities, relativeError)\n",
      "     |      Calculates the approximate quantiles of numerical columns of a\n",
      "     |      :class:`DataFrame`.\n",
      "     |      \n",
      "     |      The result of this algorithm has the following deterministic bound:\n",
      "     |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      "     |      probability `p` up to error `err`, then the algorithm will return\n",
      "     |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      "     |      close to (p * N). More precisely,\n",
      "     |      \n",
      "     |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      "     |      \n",
      "     |      This method implements a variation of the Greenwald-Khanna\n",
      "     |      algorithm (with some speed optimizations). The algorithm was first\n",
      "     |      present in [[https://doi.org/10.1145/375663.375670\n",
      "     |      Space-efficient Online Computation of Quantile Summaries]]\n",
      "     |      by Greenwald and Khanna.\n",
      "     |      \n",
      "     |      Note that null values will be ignored in numerical columns before calculation.\n",
      "     |      For columns only containing null values, an empty list is returned.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col: str, tuple or list\n",
      "     |          Can be a single column name, or a list of names for multiple columns.\n",
      "     |      \n",
      "     |          .. versionchanged:: 2.2\n",
      "     |             Added support for multiple columns.\n",
      "     |      probabilities : list or tuple\n",
      "     |          a list of quantile probabilities\n",
      "     |          Each number must belong to [0, 1].\n",
      "     |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      "     |      relativeError : float\n",
      "     |          The relative target precision to achieve\n",
      "     |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      "     |          could be very expensive. Note that values greater than 1 are\n",
      "     |          accepted but give the same result as 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list\n",
      "     |          the approximate quantiles at the given probabilities. If\n",
      "     |          the input `col` is a string, the output is a list of floats. If the\n",
      "     |          input `col` is a list or tuple of strings, the output is also a\n",
      "     |          list, but each element in it is a list of floats, i.e., the output\n",
      "     |          is a list of list of floats.\n",
      "     |  \n",
      "     |  corr(self, col1, col2, method=None)\n",
      "     |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      "     |      Currently only supports the Pearson Correlation Coefficient.\n",
      "     |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |      method : str, optional\n",
      "     |          The correlation method. Currently only supports \"pearson\"\n",
      "     |  \n",
      "     |  cov(self, col1, col2)\n",
      "     |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      "     |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column\n",
      "     |      col2 : str\n",
      "     |          The name of the second column\n",
      "     |  \n",
      "     |  crosstab(self, col1, col2)\n",
      "     |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      "     |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      "     |      non-zero pair frequencies will be returned.\n",
      "     |      The first column of each row will be the distinct values of `col1` and the column names\n",
      "     |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      "     |      Pairs that have no occurrences will have zero as their counts.\n",
      "     |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col1 : str\n",
      "     |          The name of the first column. Distinct items will make the first item of\n",
      "     |          each row.\n",
      "     |      col2 : str\n",
      "     |          The name of the second column. Distinct items will make the column names\n",
      "     |          of the :class:`DataFrame`.\n",
      "     |  \n",
      "     |  freqItems(self, cols, support=None)\n",
      "     |      Finding frequent items for columns, possibly with false positives. Using the\n",
      "     |      frequent element count algorithm described in\n",
      "     |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      "     |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cols : list or tuple\n",
      "     |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      "     |          strings.\n",
      "     |      support : float, optional\n",
      "     |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      "     |          The support must be greater than 1e-4.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This function is meant for exploratory data analysis, as we make no\n",
      "     |      guarantee about the backward compatibility of the schema of the resulting\n",
      "     |      :class:`DataFrame`.\n",
      "     |  \n",
      "     |  sampleBy(self, col, fractions, seed=None)\n",
      "     |      Returns a stratified sample without replacement based on the\n",
      "     |      fraction given on each stratum.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      col : :class:`Column` or str\n",
      "     |          column that defines strata\n",
      "     |      \n",
      "     |          .. versionchanged:: 3.0\n",
      "     |             Added sampling by a column of :class:`Column`\n",
      "     |      fractions : dict\n",
      "     |          sampling fraction for each stratum. If a stratum is not\n",
      "     |          specified, we treat its fraction as zero.\n",
      "     |      seed : int, optional\n",
      "     |          random seed\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a new :class:`DataFrame` that represents the stratified sample\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from pyspark.sql.functions import col\n",
      "     |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      "     |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      "     |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      "     |      +---+-----+\n",
      "     |      |key|count|\n",
      "     |      +---+-----+\n",
      "     |      |  0|    3|\n",
      "     |      |  1|    6|\n",
      "     |      +---+-----+\n",
      "     |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      "     |      33\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DataFrame', 'DataFrameNaFunctions', 'DataFrameStatFunction...\n",
      "\n",
      "FILE\n",
      "    /home/bigdatapedia/spark/python/pyspark/sql/dataframe.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import dataframe\n",
    "help(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a98b3-e83f-40cf-89ba-7af5eb0c3770",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25f40f4-8512-4e58-8f82-440da11bffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1| Edward|\n",
      "|  2|Praveen|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(1,'Edward'),(2,'Praveen')]\n",
    "\n",
    "schema = ['id','name']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46178e28-d35a-463e-a124-0310cf42fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df0a3d4-6562-4cf2-a10b-e63e64c504ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.readwriter.DataFrameWriter"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58befd39-7a3e-4ce2-9efb-7193979ca8e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str\n",
      "        the path in any Hadoop supported file system\n",
      "    mode : str, optional\n",
      "        specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      "            exists.\n",
      "    \n",
      "    compression : str, optional\n",
      "        compression codec to use when saving to file. This can be one of the\n",
      "        known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      "        snappy and deflate).\n",
      "    sep : str, optional\n",
      "        sets a separator (one or more characters) for each field and value. If None is\n",
      "        set, it uses the default value, ``,``.\n",
      "    quote : str, optional\n",
      "        sets a single character used for escaping quoted values where the\n",
      "        separator can be part of the value. If None is set, it uses the default\n",
      "        value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n",
      "    escape : str, optional\n",
      "        sets a single character used for escaping quotes inside an already\n",
      "        quoted value. If None is set, it uses the default value, ``\\``\n",
      "    escapeQuotes : str or bool, optional\n",
      "        a flag indicating whether values containing quotes should always\n",
      "        be enclosed in quotes. If None is set, it uses the default value\n",
      "        ``true``, escaping all values containing a quote character.\n",
      "    quoteAll : str or bool, optional\n",
      "        a flag indicating whether all values should always be enclosed in\n",
      "        quotes. If None is set, it uses the default value ``false``,\n",
      "        only escaping values containing a quote character.\n",
      "    header : str or bool, optional\n",
      "        writes the names of columns as the first line. If None is set, it uses\n",
      "        the default value, ``false``.\n",
      "    nullValue : str, optional\n",
      "        sets the string representation of a null value. If None is set, it uses\n",
      "        the default value, empty string.\n",
      "    dateFormat : str, optional\n",
      "        sets the string that indicates a date format. Custom date formats follow\n",
      "        the formats at\n",
      "        `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      "        This applies to date type. If None is set, it uses the\n",
      "        default value, ``yyyy-MM-dd``.\n",
      "    timestampFormat : str, optional\n",
      "        sets the string that indicates a timestamp format.\n",
      "        Custom date formats follow the formats at\n",
      "        `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      "        This applies to timestamp type. If None is set, it uses the\n",
      "        default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      "    ignoreLeadingWhiteSpace : str or bool, optional\n",
      "        a flag indicating whether or not leading whitespaces from\n",
      "        values being written should be skipped. If None is set, it\n",
      "        uses the default value, ``true``.\n",
      "    ignoreTrailingWhiteSpace : str or bool, optional\n",
      "        a flag indicating whether or not trailing whitespaces from\n",
      "        values being written should be skipped. If None is set, it\n",
      "        uses the default value, ``true``.\n",
      "    charToEscapeQuoteEscaping : str, optional\n",
      "        sets a single character used for escaping the escape for\n",
      "        the quote character. If None is set, the default value is\n",
      "        escape character when escape and quote characters are\n",
      "        different, ``\\0`` otherwise..\n",
      "    encoding : str, optional\n",
      "        sets the encoding (charset) of saved csv files. If None is set,\n",
      "        the default UTF-8 charset will be used.\n",
      "    emptyValue : str, optional\n",
      "        sets the string representation of an empty value. If None is set, it uses\n",
      "        the default value, ``\"\"``.\n",
      "    lineSep : str, optional\n",
      "        defines the line separator that should be used for writing. If None is\n",
      "        set, it uses the default value, ``\\\\n``. Maximum length is 1 character.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.write.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f6709a-9069-4c30-a73d-ec5ce254d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.csv(path='/user/bigdatapedia/data/tmp/',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20801ce-9c90-453d-96a5-f23f57ad929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1| Edward|\n",
      "|  2|Praveen|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check whether the file created or not\n",
    "spark.read.csv('/user/bigdatapedia/data/tmp/',header=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc808bc-4bf9-47eb-83d2-fa578d0545bf",
   "metadata": {},
   "source": [
    "**Appending columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c23aeee7-4bef-41bf-8b02-aa62d0d6a105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.csv(path='/user/bigdatapedia/data/tmp/',header=True,mode='append') # mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c8785c-5e2a-4a5b-8d44-7bf06dfb5619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1| Edward|\n",
      "|  2|Praveen|\n",
      "|  1| Edward|\n",
      "|  2|Praveen|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/user/bigdatapedia/data/tmp/',header=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a83b5-26d4-48f8-ae26-6dea5c90ae33",
   "metadata": {},
   "source": [
    "**Overwrite Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee3891e-4a07-4197-a8ef-e6c6bebbb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(path='/user/bigdatapedia/data/tmp/',header=True,mode='overwrite') # mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15cc29cb-c9fd-43ea-86bd-11a761db1589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1| Edward|\n",
      "|  2|Praveen|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 45832)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/bigdatapedia/spark/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/bigdatapedia/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/home/bigdatapedia/spark/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/bigdatapedia/spark/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/user/bigdatapedia/data/tmp/',header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9dca1-ba56-4a0f-88eb-3ceed39bcbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
